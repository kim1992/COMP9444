<!DOCTYPE html PUBLIC "-//IETF//DTD HTML 2.0//EN">
<!-- saved from url=(0055)https://www.cse.unsw.edu.au/~cs9444/18s2/hw2/index.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
<title>COMP9444 Assignment 2</title>
</head>
<body lang="EN">
<h2 align="center">COMP9444 Neural Networks and Deep Learning</h2>
<h2 align="center">Session 2, 2018</h2>
<h3 align="center">Project 2 - Recurrent Networks and Sentiment Classification</h3>
<p align="center">
Due: Sunday 23 September, 23:59 pm
<br align="center">
Marks: 15% of final assessment
</p><p>

</p><h3>Introduction</h3>

You should now have a good understanding of the internal dynamics of
TensorFlow and how to implement, train and test various network architectures.

In this assignment we will develop a classifier able to detect the sentiment
of movie reviews. Sentiment classification is an active area of research.
Aside from improving performance of systems like Siri and Cortana,
sentiment analysis is very actively utilized in the finance industry, where
sentiment is required for automated trading on news snippits and
press releases.

<p>

</p><h3>Preliminaries</h3>

Before commencing this assignment, you should download and
install TensorFlow, and the appropriate Python version.
It is also helpful to have completed the
<a href="https://www.tensorflow.org/tutorials/word2vec">Word Embeddings</a> and
<a href="https://www.tensorflow.org/tutorials/recurrent">Recurrent Neural Networks</a>
tutorials located on the TensorFlow website.
<p>
You will need to run TensorFlow 1.9 for this assignment.
The best option is to run it on your own laptop, either
in CPU or GPU mode.
We expect that most students will be using CPU mode,
and the assignment can definitely be completed in this mode.
However, the training will run significantly faster if you have an
appropriate GPU in your machine, and you manage to configure
TensorFlow to make use of it.
Instructions for setting up TF-GPU can be found
<a href="https://www.tensorflow.org/install/install_linux#NVIDIARequirements">here</a>, and there is a useful forum post
<a href="https://webcms3.cse.unsw.edu.au/COMP9444/18s2/forums/2701749">here</a>.
</p><p></p>
Another option is to make use of <a href="https://colab.research.google.com/">google colabs</a>.
This will allow you to run jupyter notebooks online, but with the ability to hook into a K80 GPU.
<p></p>
If you are physically sitting at a CSE lab machine, you can access
TensorFlow 1.9 after typing "run64".
Note however that this may run quite slowly (because it is using a 64-bit emulator and in CPU mode).
<p>

</p><h3>Provided Files</h3>

Once you are ready to begin, download the necessary files in <a href="http://www.cse.unsw.edu.au/~cs9444/18s2/hw2/Assignment2.zip">Assignment2.zip</a>
<p>
Unzip this archive by typing
</p><p>
<code>unzip Assignment2.zip</code>
</p><p>
You should then see the following files:
</p><blockquote>
<table>
    <tbody><tr><td valign="top"><code>data/ &nbsp;</code></td><td>Directory containing the training and evaluation datasets.</td></tr>
<tr><td valign="top"><code>implementation.py &nbsp;</code></td><td>This is a skeleton file for your code. The assignment should be completed by modifying this file.</td></tr>
<tr><td valign="top"><code>runner.py</code></td><td>This file is a wrapper around implementation.py and contains a large amount of pre-implemented functionality. An unedited version of this file must be used to generate your final model. </td></tr>
    <tr><td valign="top"><code>glove.6B.50d.txt</code></td><td>This is a text file containing the embedding vectors used in this assignment. Word embeddings have been shown to improve the performance of many NLP
models by converting words from character arrays to vectors that
contain semantic information of the word itself. In this assignment,
we use GloVe embeddings, you can read more about them <a href="https://nlp.stanford.edu/projects/glove/">here</a>.</td></tr>
</tbody></table>
</blockquote>

<h3>Dataset</h3>

The training dataset contains a series of movie reviews scraped from the IMDB
website.
There are no more than 30 reviews for any one specific movie.
The "data" directory contains two sub-directories, "train" and "validate".
Each of these contains two sub-directories, "pos" and "neg".

These directories contain the raw reviews in plain text form.
The "train" directory contains 12500 positive and 12500 negative reviews;
the "validate" directory contains 1000 positive and 1000 negative reviews.
Each review is confined to the first line of its associated text file,
with no line breaks.
<p>

For evaluation, we will run your model against a third dataset "test" that has not been
made available to you. If contains additional reviews in the same format.
For this reason you should be very careful to avoid
overfitting - your model could report 100% training accuracy but
completely fail on unseen reviews. There are various ways to prevent
this such as judicious use of dropout, splitting the data into a
training and validation set, etc.


</p><h3>Groups</h3>

This assignment may be done individually, or in groups of two students.
Details about how to form and register groups will be posted shortly
on the <a href="https://www.cse.unsw.edu.au/~cs9444/18s2/hw2/faq.shtml">FAQ</a>

<h3>Tasks</h3>

While the original, unchanged version of <code>runner.py</code>
must be used for your final submission,
you are encouraged to make changes to this file during development.
You may want to track additional tensors in tensorboard, or serialize
training data so that the word embedding is not called on each run.
<p>
The code to load the review files has already been written for you. However
it is essential to perform some level of preprocessing on this text
prior to feeding it into your model. Because the GloVe embeddings are
all in lowercase, you should convert all reviews to lowercase, and
also strip punctuation. You may want to do additional preprocessing by
stripping out unnecessary words etc. You should examine any avenue you
can think of to reduce superfluous data that you will feed into your
model.
</p><p>
Attempt to train a model by running <code>python runner.py train</code>. This will create local copies of the data and embedding structures for
faster subsequent loading, but will ultimately fail due to <code>implementation.py</code> being empty.
The goal of this assignment is to add code to <code>implementation.py</code> in order to train and submit a neural network capable of classifying the sentiment of the provided reviews with
a high level of accuracy.

In this assignment, unlike assignment 1, the network structure is not specified,
 and you will be assessed based on the performance of your final classifier.

There are very few constraints on your model - however for this assignment you may <b>NOT</b> make calls to the following parts of the API;
        </p><ul>
            <li><code>tf.contrib.eager</code> By extension: do not follow any example that uses the following call at any point in the code. <code>tf.enable_eager_execution()</code>. </li>

            <li><code>tf.keras</code></li>
            <li>Anything to do with estimators (<code>tf.estimator</code>)</li>
        </ul>
Calls from these sections of the API will result in us not being able to run your model and for you to receive a mark of 0 for that section of the assignment.
It is suggested to call functions from:
<ul>
    <li><code>tf.layers</code> (this is mostly wrappers for <code>tf.nn</code> but with nicer parameter structuring)</li>
    <li><code>tf.nn</code></li>
</ul>
Calls from other parts of the API such as <code>tf.math</code> are allowed but discouraged - there is probably an easier way to do what you are trying to do.

<p></p>
 During testing, we will load your saved network from a TensorFlow checkpoint
  (see: <a href="https://www.tensorflow.org/programmers_guide/saved_model">the
    TensorFlow programmers guide to saved models</a>). To allow
  our test code to find the correct path of the graph to connect to, the
  following naming requirements must be implemented.
<ol>
    <li>Input placeholder: <code>name="input_data"</code></li>
    <li>labels placeholder: <code>name="labels"</code></li>
    <li>accuracy tensor: <code>name="accuracy"</code></li>
    <li>loss tensor: <code>name="loss"</code></li>
</ol>
If your code does not meet these requirements it cannot be marked and will
be recorded as incomplete. You can verify your naming is correct by running <code>runner.py</code> in eval mode. If it
succeeds it was able to find all relevant tensors.

<h3>Assessment</h3>

This assignment will be marked on functionality in the first instance.
This table gives a general indication of what kind of mark you can expect,
based on the model accuracy achieved for the withheld test set:

<blockquote>
<table>
<tbody><tr><th>Accuracy</th><th>Mark</th></tr>
<tr><td align="center">0.6&nbsp;&nbsp;</td><td>&nbsp;8 marks</td></tr>
<tr><td align="center">0.65</td><td>&nbsp;9 marks</td></tr>
<tr><td align="center">0.7&nbsp;&nbsp;</td><td>10 marks</td></tr>
<tr><td align="center">0.75</td><td>11 marks</td></tr>
<tr><td align="center">0.8&nbsp;&nbsp;</td><td>12 marks</td></tr>
<tr><td align="center">0.85</td><td>13 marks</td></tr>
<tr><td align="center">0.9&nbsp;&nbsp;</td><td>14 marks</td></tr>
<tr><td align="center">0.95</td><td>15 marks</td></tr>
</tbody></table>
</blockquote>

Submissions failing to achieve 0.6 accuracy will be assessed by
humans and assigned a mark between 0 and 7.

<h3>Code Overview</h3>

The below section provides an outline to the structure of the provided code, however it is not exhaustive. Reading
and understanding the functionality of the provided code is part of the assignment.
<p></p>
<h4><code>runner.py</code></h4>
    This file allows for three modes of operation, "train", "eval" and "test". The first two can be used during development,
    while "test" will be used by us for marking.

    <p></p>
"train" calls functions to load the data, and convert it to embedded form.
It then trains the model defined in implementation.py, performs tensorboard logging,
and saves the model to disk every 10000 iterations. These model files are saved in a created <code>checkpoints</code>
directory, and should consist of a
<code>checkpoint
</code> file, plus three files ending in the extentions
<code>.data-00000-of-00001</code>, <code>.index</code> and <code>.meta</code>
It also prints loss
values to stdout every 50 iterations. While an unedited version of this file must be used to train your final model,
during development you are encouraged to make modifications. You may wish to display validation accuracy on your
tensorboard plots, for example.
<p></p>
"eval" evaluates the latest model checkpoint present in the local <code>checkpoints</code> directory and prints the final
accuracy to the console. You should not modify this code.

<p></p>
You may note that in both train and eval mode, the data is first fed through the <code>load_data()</code> method, which
in turn calls the <code>preprocess(review)</code> function that you will define. This is to ensure your preprocessing is
consistent across all runs. In otherwords, whatever transformations you apply to the data during training will also
be applied during evaluation and testing.

.

<p>
Further explanation of the functionality of this file appears in comments.
</p><p>

</p><h4><code>implemention.py</code></h4>
<p>
This is where you should implement your solution. This file contains
    two functions: <code>preprocess()</code>, and <code>define_graph()</code>
</p><p>

<code>preprocess(review)</code> is called whenever a review is loaded from text, prior to
being converted into embedded form. You can do anything here that is manipulation
    at a string level, e.g.
    </p><ul>
    <li>removing stop words</li>
    <li>stripping/adding punctuation</li>
    <li>changing case</li>
    <li>word find/replace</li>
    <li>paraphrasing</li>
</ul>
Note that this shouldn't be too complex - it's main purpose is to clean data for your actual model, not to be a
model in and of itself.
<p>
<code>define_graph()</code> is where you
    should define your model. You will need to define placeholders, for the input and labels,
    Note that the input is not strings of words, but the strings after the embedding lookup
    has been applied (i.e. arrays of floats).
 To ensure your model is sufficiently general (so as to achieve
    the best test accuracy) you should experiment with regularization
    techniques such as dropout. This is where you must also provide the
    correct names for your placeholders and variables.


</p><p>
There are two variables which you should
    experiment with changing. <code>BATCH_SIZE</code> defines the size of the batches that will
    be used to train the model in <code>runner.py</code> and <code>MAX_WORDS_IN_REVIEW</code> determines the maximum number for words that
are considered in each sample. Both may have a
    significant effect on model performance.

</p><h3>Visualizing Progress</h3>

In addition to the output of &nbsp;<code>runner.py</code>, you can view the progress
of your models using the tensorboard logging included in that file.
To view these logs, run the following command from the source directory:
<pre>tensorboard --logdir=./tensorboard
</pre>
<ol>
 <li> open a Web browser and navigate to &nbsp;<code>http://localhost:6006</code>
 </li><li> you should be able to see a plot of the loss and accuracies in
    TensorBoard under the "scalars" tab</li>
</ol>
    Make sure you are in the same directory from which
<code>runner.py</code> is running.
   For this assignment, tensorboard is an extremely useful tool and you
should endeavor to get it running. A good resource is
<a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">here</a>
for more information.

<h3>Submission</h3>

You need to submit six files:
<ul>
    <li>Your complete <code>implementation.py</code> file</li>
    <li><code>report.pdf</code> A maximum single page document containing a description of the design choices you made and why.
    </li><li>the (final) checkpoint files generated by an unedited <code>runner.py</code></li>
</ul>
If these files are in a directory by themselves, you can submit by typing:
<p>
<code>give cs9444 hw2 implementation.py report.pdf *.data* *.index *.meta checkpoint
</code>
</p><p>
    It is your responsibility to ensure the model can be run. The easiest way to check is to ensure running <code>python runner.py eval</code>
    functions correctly. Models that cannot be run will be assigned 0 marks. We will not manually review your code and attempt to correct errors
and assign part marks.
    </p><p></p>
You can submit as many times as you like - later submissions
will overwrite earlier ones, and submissions by either group member
will overwrite those of the other. You can check that your submission
has been received by using the following command:
<p>
<code>9444 classrun -check</code>
</p><p>
The submission deadline for both Stage 1 and Stage 2 is Sunday 23 September, 23:59.<br>
15% penalty will be applied to the (maximum) mark
for every 24 hours late after the deadline.
</p><p>

Additional information may be found in the
<a href="https://www.cse.unsw.edu.au/~cs9444/18s2/hw2/faq.shtml">FAQ</a>
and will be considered as part of the specification for the project. You should check this page regularly.
</p><p>
If you believe you are getting an error due to a bug in the specification or provided code,
you can post to the
forums on the course Web page. Please only
post queries after you have made a reasonable effort to resolve the problem yourself. If you have a generic request for help
you should attend one of the lab consultation sessions during the week.
</p><p>

</p><p>
This assignment will be marked on functionality in the first instance.
You should always adhere to good coding practices and style.
In general, a program that attempts a substantial
part of the job but does that part correctly
will receive more marks than one attempting to do
the entire job but with many errors.


</p><h3>Plagiarism Policy</h3>
<p>
Your program must be entirely your own work.
Plagiarism detection software will be used to compare all submissions pairwise
and serious penalties will be applied, particularly in the case
of repeat offences.
</p><p>
<b>DO NOT COPY FROM OTHERS; DO NOT ALLOW ANYONE TO SEE YOUR CODE</b>
</p><p>
Please refer to the
<a href="https://student.unsw.edu.au/plagiarism">UNSW Policy on Academic Integrity and Plagiarism</a>
if you require further clarification on this matter.
</p><p>
</p><p>
Good luck!
<br>
</p><hr>


</body></html>